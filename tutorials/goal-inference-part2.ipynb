{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling inference with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference algorithms we gave for goal inference in the previous notebook don't seem very intelligent---they basically boil down to randomly guessing scenarios and hoping that one happens to match the dataset. There are a number of approaches for creating more efficient inference algorithms. We will focus on one approach based on using neural networks to speed up inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addprocs(4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Gen\n",
    "@everywhere using Gen;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the relevant code from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere include(\"resources/goals/scene.jl\")\n",
    "@everywhere include(\"resources/goals/path_planner.jl\")\n",
    "@everywhere include(\"resources/goals/uniform_2d.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere include(\"resources/goals/agent_waypoint_model.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"resources/goals/rendering.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detour_dataset = [\n",
    "    Point(9.59825,8.92063)\n",
    "    Point(21.8936,9.54817)\n",
    "    Point(30.9534,10.8819)\n",
    "    Point(43.1137,9.75395)\n",
    "    Point(48.8929,10.4189)\n",
    "    Point(46.0282,21.7662)\n",
    "    Point(35.0281,25.9994)\n",
    "    Point(27.2084,33.5729)\n",
    "    Point(20.1662,39.9398)\n",
    "    Point(18.7309,50.0026)\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's understand in a bit more detail why the importance sampling inference algorithm was slow when applied to the new model with the waypoint.\n",
    "Suppose we knew the right waypoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace = ProgramTrace()\n",
    "constrain!(trace, \"use-waypoint\", true)\n",
    "constrain!(trace, \"waypoint\", Point(50, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the baseline importance sampling algorithm gives reasonable inferences with fewer samples. Recall that 1024 samples within the importance sampling algorithm were needed for reasonable inferences, without knowledge of the waypoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_samples_list = [1, 4, 32]\n",
    "figure = Figure(num_rows=1, num_cols=length(num_samples_list),\n",
    "                width=900, height=300, trace_width=100, trace_height=100,\n",
    "                margin_top=20, titles=map((n)-> \"Importance sampling ($n samples)\", num_samples_list))\n",
    "here(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSS(\"\"\"\n",
    "    #$(id(figure)) .path.recorded { visibility: hidden; }\n",
    "    #$(id(figure)) .path.constrained { visibility: visible; }\n",
    "    #$(id(figure)) .path_segments { visibility: hidden; }\n",
    "    #$(id(figure)) .destination { fill-opacity: 0.5; }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constrain!(trace, \"start\", Point(10, 10))\n",
    "for (i, point) in enumerate(detour_dataset)\n",
    "    constrain!(trace, \"x$i\", point.x)\n",
    "    constrain!(trace, \"y$i\", point.y)\n",
    "end\n",
    "renderer = JupyterInlineRenderer(\"agent_model_renderer\", Dict(\"destination\" => \"overlay\"))\n",
    "num_approximate_samples = 50\n",
    "for (i, num_samples) in enumerate(num_samples_list)\n",
    "    attach(renderer, id(figure => i))\n",
    "    for j=1:num_approximate_samples\n",
    "        output_sample = agent_waypoint_model_importance_sampling(trace, num_samples)\n",
    "        render(renderer, output_sample)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With knowledge of the waypoint, we get reasonable inferences from importance sampling with just 32 samples. We use this idea by training a neural network to make informed guesses about the waypoint, given the observed data as its input. We train the neural network on many independent unconstrained executions of the program, each of which generates a trace containing both latent variables and a dataset. The neural network will be trained to predict the waypoint given the dataset. This neural network will then be used to accelerate inference for any observed data we encounter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"resources/goals/neural.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere @program waypoint_predictor_network(features::Vector{Float64}, num_hidden_units::Int) begin\n",
    "\n",
    "    # parameters of the network, with their initial values prior to training\n",
    "    W_hidden = @e(randn(num_hidden_units, length(features)), \"W-hidden\")\n",
    "    b_hidden = @e(randn(num_hidden_units), \"b-hidden\")\n",
    "    W_output_x_mu = @e(randn(num_hidden_units), \"W-output-x-mu\")\n",
    "    b_output_x_mu = @e(randn(), \"b-output-x-mu\")\n",
    "    W_output_x_log_std = @e(randn(num_hidden_units), \"W-output-x-log-std\")\n",
    "    b_output_x_log_std = @e(randn(), \"b-output-x-log-std\")\n",
    "    W_output_y_mu = @e(randn(num_hidden_units), \"W-output-y-mu\")\n",
    "    b_output_y_mu = @e(randn(), \"b-output-y-mu\")\n",
    "    W_output_y_log_std =  @e(randn(num_hidden_units), \"W-output-y-log-std\")\n",
    "    b_output_y_log_std = @e(randn(), \"b-output-y-log-std\")\n",
    "\n",
    "    # compute the hidden layer values\n",
    "    hidden = sigmoid(W_hidden * features + b_hidden)\n",
    "\n",
    "    # sample the x-coordinate of waypoint prediction\n",
    "    x_mu = W_output_x_mu' * hidden + b_output_x_mu\n",
    "    x_std = exp(W_output_x_log_std' * hidden + b_output_x_log_std)\n",
    "    output_x = @g(normal(x_mu, x_std), \"output-x\")\n",
    "\n",
    "    # sample the y-coordinate of waypoint prediction\n",
    "    y_mu = W_output_y_mu' * hidden + b_output_y_mu\n",
    "    y_std = exp(W_output_y_log_std' * hidden + b_output_y_log_std)\n",
    "    output_y = @g(normal(y_mu, y_std), \"output-y\")\n",
    "end\n",
    "\n",
    "# we rescale the output of the neural network to make training easier\n",
    "@everywhere function scale_coordinate{T}(x::T)\n",
    "    (x - 50.) / 100.\n",
    "end\n",
    "\n",
    "@everywhere function unscale_coordinate{T}(x::T)\n",
    "    x * 100. + 50.\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict the waypoint using the measured locations at the first 10 time pooints, and our neural network predictor will use 50 hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere num_time_steps = 10\n",
    "@everywhere num_hidden_units = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain initial values for the neural network parameters by running the neural network probabilistic program once and extracting the parameter values. This procedure also serves to identify which named values in the trace are the parameters that we which to optimize over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function make_initial_parameter_values(num_time_steps::Int, num_hidden_units::Int)\n",
    "    \n",
    "    # construct example features for the neural network (to get the\n",
    "    # dimensionality of the features)\n",
    "    trace = ProgramTrace()\n",
    "    @generate!(agent_waypoint_model(), trace)\n",
    "    example_features = construct_features(trace, num_time_steps)\n",
    "\n",
    "    # run the neural network once, sampling paramters, and extract their values\n",
    "    inference_trace = ProgramTrace()\n",
    "    @generate!(waypoint_predictor_network(example_features, num_hidden_units), inference_trace)\n",
    "    parameters = Dict{String,Any}()\n",
    "    parameters[\"W-hidden\"] = inference_trace[\"W-hidden\"]\n",
    "    parameters[\"b-hidden\"] = inference_trace[\"b-hidden\"]\n",
    "    parameters[\"W-output-x-mu\"] = inference_trace[\"W-output-x-mu\"]\n",
    "    parameters[\"b-output-x-mu\"] = inference_trace[\"b-output-x-mu\"]\n",
    "    parameters[\"W-output-x-log-std\"] = inference_trace[\"W-output-x-log-std\"]\n",
    "    parameters[\"b-output-x-log-std\"] = inference_trace[\"b-output-x-log-std\"]\n",
    "    parameters[\"W-output-y-mu\"] = inference_trace[\"W-output-y-mu\"]\n",
    "    parameters[\"b-output-y-mu\"] = inference_trace[\"b-output-y-mu\"]\n",
    "    parameters[\"W-output-y-log-std\"] = inference_trace[\"W-output-y-log-std\"]\n",
    "    parameters[\"b-output-y-log-std\"] = inference_trace[\"b-output-y-log-std\"]\n",
    "    return parameters\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines the training distribution for our amortized inference neural network. Each execution of this function returns a trace of the `agent_waypoint_model` probabilistic program, which contains locations of the agent, as well as the waypoint, and all of the other elements of an internally-coherent simulated scenario. Recall that each trace is a traing datum, that contains the input to the neural network (the measured location of the simulated drone over time), and the ground-truth value of the simulated drone's waypoint, which the neural network should predict. The loss associated with each training trace is the negative log likelihood of the true waypoint under the neural networks' predictive distribution. Minimizing the expected value of this loss function over the training distribution is equivalent to minimizing the expected Kullback-Leibler divergence from the true posterior to the network's predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function model_trace_generator()\n",
    "\n",
    "    # we are only compiling for a fixed start position\n",
    "    model_trace = ProgramTrace()\n",
    "    constrain!(model_trace, \"start\", Point(10., 10.))\n",
    "   \n",
    "    # reject samples until path planning succeeded and use-waypoint = true\n",
    "    # we are assuming that the agent does use the waypoint\n",
    "    while true\n",
    "        @generate!(agent_waypoint_model(), model_trace)\n",
    "        !model_trace[\"planning-failed\"] && model_trace[\"use-waypoint\"] && break\n",
    "    end\n",
    "    @assert model_trace[\"use-waypoint\"]\n",
    "    return model_trace\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to match up the random choices representing the waypoint in the neural network trace with the random choices representing the waypoint in the model trace. Specifically, we need to indicate how to constrain the neural network trace with the ground-truth value of the waypoint obtained from a training trace.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function constrain_waypoint_network_outputs(model_trace::ProgramTrace, network_trace::ProgramTrace)\n",
    "    @assert model_trace[\"use-waypoint\"]\n",
    "    delete!(network_trace, \"output-x\")\n",
    "    delete!(network_trace, \"output-y\")\n",
    "    waypoint = model_trace[\"waypoint\"]\n",
    "    constrain!(network_trace, \"output-x\", scale_coordinate(waypoint.x))\n",
    "    constrain!(network_trace, \"output-y\", scale_coordinate(waypoint.y))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we indicate how to construct the input to the neural network from the grond-truth model trace. We scale the coordinates from the range [0, 100] to the range [-0.5, 0.5] to make training faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function construct_features(model_trace::Trace, num_time_steps::Int)\n",
    "    xs = map((j) -> model_trace[\"x$j\"], 1:num_time_steps)\n",
    "    ys = map((j) -> model_trace[\"y$j\"], 1:num_time_steps)\n",
    "    scale_coordinate(vcat(xs, ys))\n",
    "end\n",
    "\n",
    "@everywhere function inference_input_constructor(model_trace::Trace)\n",
    "    features = construct_features(model_trace, num_time_steps)\n",
    "    (features, num_hidden_units)    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put all of the components together into the `AmortizedInferenceScheme` which is defined in `neural.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amortized_inference_scheme = AmortizedInferenceScheme(\n",
    "    \n",
    "        # generates model traces which serve as training data \n",
    "        model_trace_generator,\n",
    "    \n",
    "        # the inference program being optimized\n",
    "        waypoint_predictor_network,\n",
    "    \n",
    "        # procedure for constructing input to inference program from a model trace\n",
    "        inference_input_constructor,\n",
    "    \n",
    "        # procedure for constraining output of inference program using a model trace\n",
    "        constrain_waypoint_network_outputs\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the network. We only do a few gradient steps here for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingParams(\n",
    "        32, # minibatch size\n",
    "        10, # maximum number of ADAM SGD iterations\n",
    "        32, # number of test samples to use for evaluation at each SGD step\n",
    "        ADAMParameters(0.001, 0.9, 0.999, 1e-8) # optimization parameters\n",
    ")\n",
    "\n",
    "# intiitalize parameters\n",
    "inference_parameters = make_initial_parameter_values(num_time_steps, num_hidden_units::Int)\n",
    "\n",
    "# train is defined in neural.jl. It is a generic procedure for training in an amortized inference scheme.\n",
    "inference_parameters = train(amortized_inference_scheme, inference_parameters, training_params);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have already trained the network for 20,000 iterations (about two hours), and we load those parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inference_parameters = load_neural_network(\"resources/goals/neural_waypoint_predictor_params.json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the waypoints proposed by the trained neural network for a few  different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = 9\n",
    "figure = Figure(num_rows=3, num_cols=3, width=900, height=900, trace_width=100, trace_height=100)\n",
    "here(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSS(\"\"\"\n",
    "    #$(id(figure)) .path.recorded { visibility: visible; }\n",
    "    #$(id(figure)) .path.constrained { visibility: visible; }\n",
    "    #$(id(figure)) .path_segments { visibility: visible; }\n",
    "    #$(id(figure)) .destination { visibility: visible; }\n",
    "    #$(id(figure)) .waypoint { stroke-opacity: 0.5; }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "renderer = JupyterInlineRenderer(\"agent_model_renderer\", Dict(\"waypoint\" => \"overlay\"))\n",
    "for dataset_index=1:num_datasets    \n",
    "    model_trace = model_trace_generator()\n",
    "    for i=1:50\n",
    "        network_trace = ProgramTrace()\n",
    "        for key in keys(inference_parameters)\n",
    "            intervene!(network_trace, key, inference_parameters[key])\n",
    "        end\n",
    "        network_input = inference_input_constructor(model_trace)\n",
    "        @generate!(waypoint_predictor_network(network_input...), network_trace)\n",
    "        \n",
    "        delete!(model_trace, \"waypoint\")\n",
    "        constrain!(model_trace, \"waypoint\", Point(\n",
    "                unscale_coordinate(network_trace[\"output-x\"]),\n",
    "                unscale_coordinate(network_trace[\"output-y\"])))\n",
    "\n",
    "        attach(renderer, id(figure => dataset_index))\n",
    "        render(renderer, model_trace)\n",
    "        sleep(0.01)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see the predictions for our particular original detour dataset, and compare these to the predictions made from the uniform distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = Figure(num_rows=1, num_cols=2,\n",
    "                width=900, height=300, trace_width=100, trace_height=100,\n",
    "                margin_top=20, titles=[\"uniform proposal\", \"trained neural proposal\"])\n",
    "here(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSS(\"\"\"\n",
    "    #$(id(figure)) .path.recorded { visibility: hidden; }\n",
    "    #$(id(figure)) .path.constrained { visibility: visible; }\n",
    "    #$(id(figure)) .path_segments { visibility: hidden; }\n",
    "    #$(id(figure)) .destination { visibility: hidden; }\n",
    "    #$(id(figure)) .waypoint { stroke-opacity: 0.5; }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "renderer = JupyterInlineRenderer(\"agent_model_renderer\", Dict(\"waypoint\" => \"overlay\"))\n",
    "\n",
    "model_trace = ProgramTrace()\n",
    "constrain!(model_trace, \"start\", Point(10, 10))\n",
    "constrain!(model_trace, \"use-waypoint\", true)\n",
    "for (i, point) in enumerate(detour_dataset)\n",
    "    constrain!(model_trace, \"x$i\", point.x)\n",
    "    constrain!(model_trace, \"y$i\", point.y)\n",
    "end\n",
    "@generate!(agent_waypoint_model(), model_trace)\n",
    "\n",
    "# show uniform proposals\n",
    "attach(renderer, id(figure => 1))\n",
    "for i=1:50\n",
    "    @generate!(agent_waypoint_model(), model_trace)\n",
    "    render(renderer, model_trace)\n",
    "end\n",
    "\n",
    "# show neural proposals\n",
    "attach(renderer, id(figure => 2))\n",
    "for i=1:50\n",
    "    network_trace = ProgramTrace()\n",
    "    for key in keys(inference_parameters)\n",
    "        intervene!(network_trace, key, inference_parameters[key])\n",
    "    end\n",
    "    network_input = inference_input_constructor(model_trace)\n",
    "    @generate!(waypoint_predictor_network(network_input...), network_trace)\n",
    "            \n",
    "    delete!(model_trace, \"waypoint\")\n",
    "    constrain!(model_trace, \"waypoint\", Point(\n",
    "        unscale_coordinate(network_trace[\"output-x\"]),\n",
    "        unscale_coordinate(network_trace[\"output-y\"])))\n",
    "    \n",
    "    @generate!(agent_waypoint_model(), model_trace)\n",
    "    render(renderer, model_trace)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is able to make better-than-random predictions of the waypoint location for many of the sampled datasets, including our particular `detour_dataset` of interest. We spent the up-front computational cost of training the network, but now that we have trained it, we can apply to it any data we encounter. The idea of amortizing the cost of inference across many problem instances is called **amortized inference**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this trained neural network to speed up inference for our dataset. We use the network to make intelligent proposals for the waypoint within an importance sampling algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Distributions \n",
    "\n",
    "@everywhere function agent_waypoint_model_neural_importance_sampling(trace::ProgramTrace, num_samples::Int,\n",
    "                                                                     network_parameters::Dict{String,Any})\n",
    "    # compute the input for this data\n",
    "    network_input = inference_input_constructor(trace)\n",
    "\n",
    "    # set parameters in the network\n",
    "    network_trace = ProgramTrace()\n",
    "    for key in keys(inference_parameters)\n",
    "        intervene!(network_trace, key, inference_parameters[key])\n",
    "    end\n",
    "    propose!(network_trace, \"output-x\", Float64)\n",
    "    propose!(network_trace, \"output-y\", Float64)\n",
    "\n",
    "    traces = Vector{ProgramTrace}(num_samples)\n",
    "    scores = Vector{Float64}(num_samples)\n",
    "    for k=1:num_samples\n",
    "        \n",
    "        # fork a copy of the model trace\n",
    "        t = deepcopy(trace)\n",
    "        \n",
    "        # predict the waypoint from the neural network\n",
    "        (network_score, _) = @generate!(waypoint_predictor_network(network_input...), network_trace)\n",
    "        delete!(t, \"waypoint\")\n",
    "        waypoint = Point(\n",
    "                unscale_coordinate(network_trace[\"output-x\"]),\n",
    "                unscale_coordinate(network_trace[\"output-y\"]))\n",
    "        constrain!(t, \"waypoint\", waypoint)\n",
    "        \n",
    "        # propose the reset of the random chocies in the model by executing the model\n",
    "        # program, conditioned on the predicted waypoint\n",
    "        (model_score, _) = @generate!(agent_waypoint_model(), t)\n",
    "\n",
    "        # the score is the model score minus the neural proposal score\n",
    "        scores[k] = model_score - network_score\n",
    "        traces[k] = t\n",
    "    end\n",
    "    weights = exp.(scores - logsumexp(scores))\n",
    "    weights = weights / sum(weights)\n",
    "    \n",
    "    if !Distributions.isprobvec(weights)\n",
    "        return ProgramTrace() # no sample produced\n",
    "    end\n",
    "    \n",
    "    chosen = rand(Distributions.Categorical(weights))\n",
    "    return traces[chosen]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_list = [1, 8, 64]\n",
    "figure = Figure(num_rows=1, num_cols=length(num_samples_list),\n",
    "                width=900, height=300, trace_width=100, trace_height=100,\n",
    "                margin_top=20, titles=map((n)-> \"SIR ($n samples)\", num_samples_list))\n",
    "\n",
    "here(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSS(\"\"\"\n",
    "    #$(id(figure)) .path.recorded { visibility: hidden; }\n",
    "    #$(id(figure)) .path.constrained { visibility: visible; }\n",
    "    #$(id(figure)) .path_segments { visibility: hidden; }\n",
    "    #$(id(figure)) .destination { fill-opacity: 0.5; }\n",
    "    #$(id(figure)) .waypoint { stroke-opacity: 0.5; }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "renderer = JupyterInlineRenderer(\"agent_model_renderer\",  Dict(\"destination\" => \"overlay\", \"waypoint\" => \"overlay\"))\n",
    "\n",
    "trace = ProgramTrace()\n",
    "constrain!(trace, \"start\", Point(10., 10.))\n",
    "constrain!(trace, \"use-waypoint\", true)\n",
    "for (i, point) in enumerate(detour_dataset)\n",
    "    constrain!(trace, \"x$i\", point.x)\n",
    "    constrain!(trace, \"y$i\", point.y)\n",
    "end\n",
    "   \n",
    "num_approximate_samples = 50\n",
    "for (i, num_samples) in enumerate(num_samples_list)\n",
    "    attach(renderer, id(figure => i))\n",
    "    for j=1:num_approximate_samples\n",
    "        output_sample = agent_waypoint_model_neural_importance_sampling(trace, num_samples, inference_parameters)\n",
    "        render(renderer, output_sample)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having spent the computational resources up front to compile the neural network to predict the waypoint, we are now able to obtain reasonable inferences with just 64 samples within importance sampling. Running the neural network adds has negligible computational cost compared with running the probabilistic program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

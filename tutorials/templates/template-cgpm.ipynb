{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis with BayesDB\n",
    "\n",
    "Authored by: [Ulrich Schaechtle](www.schaechtle.com) of the MIT Probabilistic\n",
    "Computing Project (Probcomp). Prepared for in February, 2018. This is a\n",
    "generalization of the gapminder-exploratory analysis, authored by\n",
    "[Feras Saad](http://fsaad.mit.edu).\n",
    "\n",
    "This notebook,will serve as a template for  exploratory date analysis with\n",
    "BayesDB. It should work on any analysis-ready `.csv`-file.\n",
    "\n",
    "A user needs to change.\n",
    "\n",
    "This notebook will cover the following topics:\n",
    "\n",
    "- Activating `jupyter_probcomp` magics and associated libraries.\n",
    "- Customizing this notebook for your own data!\n",
    "- Defining experimental outcomes of interest.\n",
    "- Creating a BayesDB file on disk, which will store data and models.\n",
    "- Ingesting data from .csv files into BayesDB.\n",
    "- Basic data manipulation, subsampling and plotting using SQL and\n",
    "`jupyter_probcomp` magics.\n",
    "- Creating populations for database tables using the Metamodeling Language\n",
    "(MML).\n",
    "- Producing visualizations Cross-Categorization (CrossCat), the default model\n",
    "discovery method for populations in BayesDB.\n",
    "- Building an ensemble of CrossCat models, and visualizing their aggregate\n",
    "properties.\n",
    "- Using the Bayesian Query Language (BQL) to query the ensemble of CrossCat\n",
    "models for exploratory tasks, such as (i) detecting variables which are probably\n",
    "dependent, and (ii) finding database records which are probably predictive of\n",
    "one another.\n",
    "\n",
    "### Setting up the Jupyter environment\n",
    "\n",
    "The first step is to load the `jupyter_probcomp.magics` library, which provides\n",
    "BayesDB hooks for data exploration, plotting, querying, and analysis through\n",
    "this Jupyter notebook environment. The second cell allows plots from matplotlib\n",
    "and javascript to be shown inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_probcomp.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%vizgpm inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize this notebook for your data!\n",
    "\n",
    "Put a path to an analysis ready `.csv` here. If you are not sure how such a .csv\n",
    "file should be formatted -- please read the [population assembly\n",
    "tutorial](../population-assembly-tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'your-file.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining experimental outcomes of interests\n",
    "Define which variables/columns in your data tables are of particular interest.\n",
    "Those will henceforth be called `outcomes`. Ensure that the strings contain\n",
    "double qouted names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [\n",
    "    '\"x\"',\n",
    "    '\"y\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BayesDB `.bdb` file on disk\n",
    "\n",
    "First, we remove all files with the name of the current .bdb file to avoid confusion. Make sure to either rename the `.bdb` file below or comment out the next line for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f bayesian_database.bdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the `%bayesdb` magic to create a `.bdb` file on disk named\n",
    "`bayesian_database.bdb`. This file will store all the data and models created in this\n",
    "session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bayesdb bayesian_database.bdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting data from a `.csv` file into a BayesDB table\n",
    "\n",
    "All datasets that are considered analysis ready are stored in form of csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql CREATE TABLE \"data_full\" FROM '{csv_file_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to environmnt variables set in SQL-lite, we need to subsample to  ensure we have less than 1000 columns. If the cell above throw an error indicating too many columns then turn the two cells below into code cells (using the drop down menu in above in the menu bar."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(csv_file_path)\n",
    "all_columns = df.columns.tolist()\n",
    "rand_selected_columns = np.random.choice(all_columns, size=999, replace=False).tolist()\n",
    "df = df[rand_selected_columns]\n",
    "df.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%bql CREATE TABLE \"data_full\" FROM 'temp.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of the csv file is a variable, and each row is a record. We use the `CREATE TABLE` BQL query, with the pathname of the csv file, to convert the csv data into a database table named `data_full`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all datasets have missing values, and special tokens such as `NaN` or\n",
    "`NA` indicating a particular cell is missing. In most data, empty\n",
    "strings are used. To tell BayesDB to treat empty strings as SQL `NULL` we use\n",
    "the `.nullify` command, followed by the name of the table and the string `''`\n",
    "which represents missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .nullify data_full ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further repeat the exercise for the strings `NA` and `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .nullify data_full 'NA'\n",
    "%bql .nullify data_full 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If users know about any other missing values, they should edit the cells above\n",
    "\n",
    "\n",
    "### Running basic queries on the table using BQL and SQL\n",
    "\n",
    "Now that the dataset has been loaded into at table, and missing values\n",
    "converted to `NULL`, we can run standard SQL queries to explore the contents of\n",
    "the data. For example, we can select the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"data_full\" LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the total number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT COUNT(*) FROM \"data_full\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling the columns in the table\n",
    "\n",
    "The full data table (`data_full`) may contains a large number of columns.\n",
    "In this notebook, our exploratory analysis will be based on a random subsample of\n",
    "100 columns. To create the subsample, we use the `.subsample_columns` magic. The\n",
    "`--keep` flag accepts a list of column names which should be kept. We will keep \n",
    "the columns denoted as experimental outcomes, since it is the identifier for\n",
    "each record. The `--seed` flag specifies\n",
    "the random seed to create the subsample, which will ensure our analyses are\n",
    "reproducible. Finally, `data_full` is the original table, `data` is\n",
    "the name of the new table, and 100 is the number of columns to downsample to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_str = ' '.join(outcomes)\n",
    "%bql .subsample_columns --seed=8 data_full data_column_subsampled 100 --keep  {outcomes_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling the rows in the table\n",
    "\n",
    "We further may wish to downsample the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE data AS SELECT * FROM data_column_subsampled \n",
    "    ORDER BY bql_rand() LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BayesDB population for data\n",
    "\n",
    "The notion of a \"population\" is a central concept in BayesDB. For a standard\n",
    "database table, such as `data`, each column is associated with a [data\n",
    "type](https://sqlite.org/datatype3.html), which in sqlite3 are `TEXT`, `REAL`,\n",
    "`INTEGER`, and `BLOB`. For a BayesDB population, each variable is associated\n",
    "with a _statistical data type_. \n",
    "\n",
    "These statistical types, such as `NOMINAL`,\n",
    "`NUMERICAL`, `MAGNITUDE`, and `COUNTS`, specify the set of values and default\n",
    "probability distributions used for building probabilistic models of the data in\n",
    "the population. In this tutorial, we will use the `NUMERICAL` and `NOMINAL`\n",
    "statistical data types.\n",
    "\n",
    "We can use the `GUESS SCHEMA FOR <table>` command from the Metamodeling Language\n",
    "(MML) in BayesDB to guess the statistical data types of variables in the table.\n",
    "The guesses use heuristics based on the contents in the cells. The\n",
    "`num_distinct` column shows the number of unique values for that variable, and\n",
    "the `reason` column explains which heuristic was used to make the guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml GUESS SCHEMA FOR \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this case to create a population for the `data` table. The population\n",
    "schema uses the statistical types guessed by BayesDB (from the previous cell) for all\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mml\n",
    "CREATE POPULATION FOR \"data\" WITH SCHEMA (\n",
    "    -- Use the guesses from the previous cell for all variables.\n",
    "    GUESS STATTYPES OF (*);\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing joint distributions of data in the population\n",
    "\n",
    "Equipped with the statistical data types of variables in the population, we can\n",
    "now use the plotting features of BayesDB to produce scatter plots and heatmaps\n",
    "for the marginal and (pairwise) joint distributions of variables of interest.\n",
    "The `.interactive_pairplot` command requires a flag `--population=<pop>` for the\n",
    "population name, followed a BQL query. It generates pairplots of the data in all\n",
    "pairs of columns yielded by the query . Below, we have selected the experimental\n",
    "outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_str = ','.join(outcomes)\n",
    "%bql .interactive_pairplot --population=data SELECT {outcomes_str} FROM data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a generator for the population using CrossCat\n",
    "\n",
    "Now that we have created the `data` population, the next step is to analyze\n",
    "the data by building probabilistic models which explain the data generating\n",
    "process. Probabilistic data analyses in BayesDB are specified by declaring\n",
    "`GENERATOR` for a population. The default generator in BayesDB is based on\n",
    "Cross-Categorization [(Crosscat)](http://jmlr.org/papers/v17/11-392.html). The\n",
    "CrossCat generator is a Bayesian factorial mixture model which learns a full\n",
    "joint distribution over all variables in the population, using a divide-and-\n",
    "conquer approach. We will explore CrossCat more in this notebook.\n",
    "\n",
    "For now we use MML to declare the a generator for the `data` population.\n",
    "Note that that we have left the schema (there are not specific model commands or\n",
    "overrides), which will apply the built-in default model discovery strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml CREATE GENERATOR FOR \"data\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the generator, we now need to initialize `MODELS` for the\n",
    "generator. We can think of a `GENERATOR` as specifying a hypothesis space of\n",
    "explanations for the data generating process for the population, and each\n",
    "`MODEL` is a candidate hypothesis. We start by creating only 1 model, which is\n",
    "initialized __randomly__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml INITIALIZE 1 MODEL FOR \"data\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a CrossCat model\n",
    "\n",
    "As mentioned earlier, CrossCat learns the full joint distribution of all\n",
    "variables in the population using divide-and-conquer:\n",
    "\n",
    "- First, CrossCat partitions the variables into a set of _views_; all the\n",
    "variables in a particular view are modeled jointly, and two variables in\n",
    "different views are independent of one another.\n",
    "- Second, within each view, CrossCat clusters the rows using a non-parametric\n",
    "mixture model.\n",
    "\n",
    "The name Cross-Categorization is derived from this two-step process: first\n",
    "categorize the variables into views, and then categorize the rows into clusters\n",
    "within each view of variables. It is important to note that two different views\n",
    "A and B are likely to induce different clusterings of the rows.\n",
    "\n",
    "To get a sense of CrossCat's hypothesis space, we can render the hypothesis\n",
    "specified by a particular model using the `.render_crosscat [options]\n",
    "<generator> <model_number>` plotting command. The `--subsample=50` option says\n",
    "to only show a subsample of 50 rows in the rendering (even though the generator\n",
    "is modeling all rows in the `data` population); finally `data\n",
    "0` means to render the first (and only) model in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml .render_crosscat \\\n",
    "    --subsample=50  --xticklabelsize=small --yticklabelsize=xx-small --progress=True --width=64 \\\n",
    "    data 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__To view a full-size image of the rendering, either double click the image, or\n",
    "right-click and select \"Open image in new tab.\"__\n",
    "\n",
    "Again, we emphasize that the CrossCat hypothesis shown above is __randomly__\n",
    "initialized based on the two-step clustering process we have described. Each\n",
    "block of columns shows a view of dependent variables. The clusters within a view\n",
    "are demarcated using solid pink lines.\n",
    "The color of a cell shows the magnitude of the data (normalized between 0 and 1,\n",
    "where light indicates lower values and dark indices higher values).\n",
    "\n",
    "### Using BQL to query CrossCat models\n",
    "\n",
    "In the CrossCat rendering, each pair of variables is either in the same view\n",
    "(and therefore probably dependent), or in different views (and therefore\n",
    "independent). We can query the detected probable dependencies between all pairs\n",
    "of variables using the `DEPENDENCE PROBABILITY` estimator in BQL. The next query\n",
    "produces a heatmap of all pairs of dependencies. In the heatmap below, each row\n",
    "and column is a variable, and the color of a cell is a value between 0 and 1\n",
    "(lighter is nearer to 0, and darker is nearer to 1) indicating the amount of\n",
    "evidence for a predictive relationship or dependency between these two\n",
    "variables. Since we have initialized only 1 CrossCat model, each cell is exactly\n",
    "either 0 (if those variables are in different views), or 1 (variables are in the\n",
    "same view). Confirm that the blocks shown in the heatmap match up with the\n",
    "blocks of variables from the rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap ESTIMATE DEPENDENCE PROBABILITY FROM PAIRWISE VARIABLES OF data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Crosscat hypotheses using MML `ANALYZE`\n",
    "\n",
    "Now that we have initialized a CrossCat hypothesis and visualized its column and\n",
    "row clusterings, it is time to improve our initial guess by exploring the\n",
    "hypothesis space to find hypotheses that better explain the data. In particular,\n",
    "our single CrossCat model has both spurious dependencies as well as\n",
    "independencies between variables which we would expect to be depedenct (study\n",
    "the heatmap and rendering, can you locate some of these pairs?).\n",
    "\n",
    "We can improve the CrossCat model by using the MML `ANALYZE` command, which\n",
    "takes the name of a generator, an amount of iterations or seconds, and optional\n",
    "arguments. It then searches for improved hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml ANALYZE \"data\" FOR 50 ITERATIONS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the new CrossCat after running 200 steps of analysis. Study the\n",
    "dependent variables. Can you identify a \"theme\" or \"category\" which summarizes\n",
    "each view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml .render_crosscat \\\n",
    "    --subsample=50  --xticklabelsize=small --yticklabelsize=xx-small --progress=True --width=64 \\\n",
    "    data 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again visualize the probability there exists a dependence, between all\n",
    "pairs of variables, using BQL. How does this heatmap differ qualitatively from\n",
    "the dependence probability heatmap we plotted prior to running `ANALYZE`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap ESTIMATE DEPENDENCE PROBABILITY FROM PAIRWISE VARIABLES OF data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing more CrossCat models\n",
    "\n",
    "So far, we used `INITIALIZE 1 MODEL FOR data` to create a single model in\n",
    "the ensemble. As a result, all of our heatmaps (such as a variable dependencies\n",
    "and row similarities) had \"sharp\" values (either 1 or 0). Since CrossCat has a\n",
    "very large hypothesis space, we can significantly improve modeling by creating\n",
    "an ensemble of models, where each model searches the hypothesis space for\n",
    "hypotheses that fit the data well. All queries in BQL will then become weighted\n",
    "averages of the query results from each individual model in the ensemble.\n",
    "\n",
    "The `%multiprocess on` magic activates multiprocessing BayesDB which allow us to\n",
    "initialize, analyze and run queries on analyses using multiple cores on the host\n",
    "machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%multiprocess on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following MML command ensures the generator will have a total of 32 models\n",
    "in the ensemble (recall that we already initialized 1 model, so 31 new models\n",
    "will be added to the ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml INITIALIZE 10 MODELS IF NOT EXISTS FOR \"data\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we run analysis for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml ANALYZE \"data\" FOR 150 ITERATIONS (OPTIMIZED);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let use produce some renderings of the models (here we choose 5, 7 and 15).\n",
    "Where is there consensus among these three analyses? Where do they disagree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml .render_crosscat \\\n",
    "    --subsample=50  --xticklabelsize=small --yticklabelsize=x-small data 5\n",
    "%mml .render_crosscat \\\n",
    "    --subsample=50  --xticklabelsize=small --yticklabelsize=x-small data 7\n",
    "%mml .render_crosscat \\\n",
    "    --subsample=50  --xticklabelsize=small --yticklabelsize=x-small data 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring probable dependencies between variables and comparing CrossCat\n",
    "dependence probability to linear (Pearson R) correlation\n",
    "\n",
    "As mentioned earlier, all BQL queries are aggregated across the 32 analyses in\n",
    "the ensemble. We will create a table named `dependencies` which contains the\n",
    "pairwise `DEPENDENCE PROBABILITY` values between the variables in the data. The\n",
    "value of a cell (between 0 and 1) is the fraction of analyses in the ensemble\n",
    "where those two variables are detected to be probably dependent (i.e. they are\n",
    "in the same view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE dependencies AS\n",
    "ESTIMATE\n",
    "    DEPENDENCE PROBABILITY AS \"depprob\"\n",
    "FROM PAIRWISE VARIABLES OF data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random rows from the `dependencies` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"dependencies\" ORDER BY RANDOM() LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again summarize the `dependencies` table using a heatmap. Study this\n",
    "dependence heatmap, and compare it to the heatmap produced when there was only 1\n",
    "model. Which common-sense dependencies were missed by the single model, but\n",
    "identified by the ensemble as probably dependent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap SELECT name0, name1, depprob FROM dependencies;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare dependence probabilities from CrossCat to linear (Pearson r)\n",
    "correlation values, a very common technique for finding predictive\n",
    "relationships. We can compute the Pearson R (and its p-value) in BayesDB using\n",
    "the `CORRELATION` and `CORRELATION PVALUE` queries. The following cell creates a\n",
    "table named `correlations`, which contains the R and p-value for all pairs of\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE \"correlations\" AS\n",
    "ESTIMATE\n",
    "    CORRELATION AS \"correlation\",\n",
    "    CORRELATION PVALUE AS \"pvalue\"\n",
    "FROM PAIRWISE VARIABLES OF \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random rows from the `correlations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"correlations\" ORDER BY RANDOM() LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Emphasis__: There is a signficiant difference between `DEPENDENCE\n",
    "PROBABILITY`, `CORRELATION`, and `CORRELATION PVALUE`. We outline these\n",
    "differences below, which will help us make comparisons between predictive\n",
    "relationships detected by CrossCat versus Pearson correlation.\n",
    "\n",
    "- `DEPENDENCE PROBABILITY`: Returns a value between [0,1] indicating the\n",
    "__probability there exists__ a predictive relationship (statistical dependence)\n",
    "between two variables.\n",
    "\n",
    "- `CORRELATION`: Returns a value between [0,1] indicating the __strength__ of\n",
    "the linear relationsip between two variables, where 0 means no linear\n",
    "correlation, and 1 means perfect linear correlation.\n",
    "\n",
    "- `CORRELATION PVALUE`: Returns a value between (0, 1) indicating the tail\n",
    "probability of the observed correlation value between two variables, under the\n",
    "null hypothesis that the two variables have zero correlation.\n",
    "\n",
    "Based on these distinctions, there is no immediate way to numerically compare\n",
    "`DEPENDENCE PROBABILITY` with `CORRELATION/CORRELATION PVALUE`. However, it is\n",
    "possible to compare the inferences about predictive relationships that each\n",
    "method gives rise to, which we do in the next section.\n",
    "\n",
    "Let us first produce a heatmap of the raw correlation values. The following\n",
    "query shows the raw correlation values (between 0 and 1) for all pairs of\n",
    "variables where the p-value is less than 0.01 (note that we are not accounting\n",
    "for multiple-testing using e.g. Bonferroni correction). Pairs of variables where\n",
    "the p-value exceeds 0.01 (and thus the null hypothesis of independence cannot be\n",
    "rejected) are shown in gray. The sparsity of the data makes it difficult to draw\n",
    "inferences about many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap SELECT name0, name1, \"correlation\" FROM \"correlations\" WHERE \"pvalue\" < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the heatmap, and compare it to the heatmap from `DEPENDENCE\n",
    "PROBABILITY`. The patterns of dependence relationships differ significantly,\n",
    "how?\n",
    "\n",
    "We can use BQL to find variables which CrossCat believes are probably dependent,\n",
    "but correlation believes are independent (either the null hypothesis of\n",
    "independence cannot be rejected, or the correlation value is significant and\n",
    "near zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    \"name0\",\n",
    "    \"name1\",\n",
    "    \"dependencies\".\"depprob\",\n",
    "    \"correlations\".\"correlation\",\n",
    "    \"correlations\".\"pvalue\"\n",
    "FROM\n",
    "    \"dependencies\"\n",
    "    JOIN \"correlations\"\n",
    "    USING (\"name0\", \"name1\")\n",
    "WHERE\n",
    "    -- CrossCat: probability dependent.\n",
    "    \"dependencies\".\"depprob\" > 0.85\n",
    "    AND (\n",
    "    -- Correlation: cannot reject null hypothesis of independence.\n",
    "    \"correlations\".\"pvalue\" > 0.05\n",
    "    OR (\n",
    "    -- Correlation: linear relationship is significant and near zero.\n",
    "    \"correlations\".\"pvalue\" < 0.05 AND \"correlations\".\"correlation\" < 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use also BQL to find variables which CrossCat believes are probably\n",
    "independent, but correlation believes are dependent (a statistically significant\n",
    "non-zero correlation value, where we are using an R cutoff of 0.15). The\n",
    "following query shows a list of such variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    \"name0\",\n",
    "    \"name1\",\n",
    "    \"dependencies\".\"depprob\",\n",
    "    \"correlations\".\"correlation\",\n",
    "    \"correlations\".\"pvalue\"\n",
    "FROM\n",
    "    \"dependencies\"\n",
    "    JOIN \"correlations\"\n",
    "    USING (\"name0\", \"name1\")\n",
    "WHERE\n",
    "    -- CrossCat: high uncertainty about dependence probability.\n",
    "    \"dependencies\".\"depprob\" < 0.05\n",
    "    AND (\n",
    "    -- Correlation: statistically significant dependence.\n",
    "    \"correlations\".\"pvalue\" < 0.05 AND \"correlations\".\"correlation\" > 0.15)\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, we notice that linear correlation is deceived into detecting a\n",
    "dependency due to a single outlier in both cases. As a non-parametric mixture\n",
    "model, CrossCat is more robust to outliers and irregular patterns in the data,\n",
    "especially when there is insufficient evidence in the data to result in CrossCat\n",
    "reporting probable dependencies (as is the case in the two scatter plots, with\n",
    "only one data point deviating from the zero-dependence trend).\n",
    "\n",
    "Bonferroni correction for multiple testing would perhaps render the p-values of\n",
    "these correlations as statistically insignificant. However, Bonferroni is also\n",
    "highly conservative, and will cause many common-sense relationships to be\n",
    "insigificant as well under linear correlation. These design trade-offs are very\n",
    "common in drawing inferences from frequentist methods such Pearson R.\n",
    "\n",
    "Some next questions you might explore include:\n",
    "\n",
    "- For which variables do CrossCat and linear correation agree about\n",
    "dependencies?\n",
    "- Which pairs of variables have the most uncertainty about their dependence\n",
    "probability (a dependence probability value of 0.5 represents the most\n",
    "uncertainy, or a light green color)?\n",
    "\n",
    "\n",
    "### Exploring the clustering of the with respect to experimental outcomes\n",
    "\n",
    "Recall that in addition to learning a clustering of variables, CrossCat\n",
    "additionally learns a clustering of the rows within each view. These clusters\n",
    "are separated using pink lines in the CrossCat rendering. We can use the\n",
    "`SIMILARITY IN THE CONTEXT OF <variable>` query in BQL to study CrossCat's row\n",
    "partition in the view of `<variable>`.\n",
    "\n",
    "In the heatmaps below, each row and column is a row in the data table, and the value \n",
    "of a cell (between 0 and 1) indicates the probability that those two rows are\n",
    "relevant for formulating predictions about each other.  We produces one heatmap\n",
    "for each experimental outcome. Do these clusterings make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for outcome in outcomes:\n",
    "    fig, ax = plt.subplots()\n",
    "    %bql .heatmap --label0=rowid --table=data ESTIMATE SIMILARITY IN THE CONTEXT OF {outcome}\\\n",
    "        FROM PAIRWISE data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive version\n",
    "\n",
    "We can't plot all the interactive plots automatically because the javascript objects will overwrite each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap --label0=rowid --table=data \\\n",
    "    ESTIMATE SIMILARITY IN THE CONTEXT OF {outcomes[0]} FROM PAIRWISE data;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

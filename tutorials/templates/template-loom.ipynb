{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis with BayesDB\n",
    "\n",
    "Authored by: [Ulrich Schaechtle](www.schaechtle.com) of the MIT Probabilistic\n",
    "Computing Project (Probcomp). Prepared for in February, 2018. This is a\n",
    "generalization of the gapminder-exploratory analysis, authored by\n",
    "[Feras Saad](http://fsaad.mit.edu).\n",
    "\n",
    "This notebook,will serve as a template for  exploratory date analysis with\n",
    "BayesDB. It should work on any analysis-ready `.csv`-file.\n",
    "\n",
    "A user needs to change.\n",
    "\n",
    "This notebook will cover the following topics:\n",
    "\n",
    "- Activating `jupyter_probcomp` magics and associated libraries.\n",
    "- Customizing this notebook for your own data!\n",
    "- Defining experimental outcomes of interest.\n",
    "- Creating a BayesDB file on disk, which will store data and models.\n",
    "- Ingesting data from .csv files into BayesDB.\n",
    "- Basic data manipulation, subsampling and plotting using SQL and\n",
    "`jupyter_probcomp` magics.\n",
    "- Creating populations for database tables using the Metamodeling Language\n",
    "(MML).\n",
    "- Producing visualizations Cross-Categorization (CrossCat), the default model\n",
    "discovery method for populations in BayesDB.\n",
    "- Building an ensemble of CrossCat models, and visualizing their aggregate\n",
    "properties.\n",
    "- Using the Bayesian Query Language (BQL) to query the ensemble of CrossCat\n",
    "models for exploratory tasks, such as (i) detecting variables which are probably\n",
    "dependent, and (ii) finding database records which are probably predictive of\n",
    "one another.\n",
    "\n",
    "### Setting up the Jupyter environment\n",
    "\n",
    "The first step is to load the `jupyter_probcomp.magics` library, which provides\n",
    "BayesDB hooks for data exploration, plotting, querying, and analysis through\n",
    "this Jupyter notebook environment. The second cell allows plots from matplotlib\n",
    "and javascript to be shown inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_probcomp.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%vizgpm inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize this notebook for your data!\n",
    "\n",
    "Put a path to an analysis ready `.csv` here. If you are not sure how such a .csv\n",
    "file should be formatted -- please read the [population assembly\n",
    "tutorial](../population-assembly-tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'your-file.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining experimental outcomes of interests\n",
    "Define which variables/columns in your data tables are of particular interest.\n",
    "Those will henceforth be called `outcomes`. Ensure that the strings contain\n",
    "double qouted names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = [\n",
    "    '\"x\"',\n",
    "    '\"y\"',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BayesDB `.bdb` file on disk\n",
    "\n",
    "First, we remove all files with the name of the current .bdb file to avoid confusion. Make sure to either rename the `.bdb` file below or comment out the next line for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f bayesian_database.bdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the `%bayesdb` magic to create a `.bdb` file on disk named\n",
    "`bayesian_database.bdb`. This file will store all the data and models created in this\n",
    "session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bayesdb bayesian_database.bdb\n",
    "bdb = %get_bdb\n",
    "import os\n",
    "os.environ['LOOM_VERBOSITY'] = '0'\n",
    "import bayeslite\n",
    "from bayeslite.backends.loom_backend import LoomBackend\n",
    "from bayeslite import bayesdb_register_backend\n",
    "bayesdb_register_backend(bdb, LoomBackend(os.path.abspath('loom_files/')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingesting data from a `.csv` file into a BayesDB table\n",
    "\n",
    "All datasets that are considered analysis ready are stored in form of csv files. Due to environmnt variables set in SQL lite, we need to subsample to to ensure we have less than 1000 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql CREATE TABLE \"data_full\" FROM '{csv_file_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to environmnt variables set in SQL-lite, we need to subsample to  ensure we have less than 1000 columns. If the cell above throw an error indicating too many columns then turn the two cells below into code cells (using the drop down menu in above in the menu bar."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(csv_file_path)\n",
    "all_columns = df.columns.tolist()\n",
    "rand_selected_columns = np.random.choice(all_columns, size=999, replace=False).tolist()\n",
    "df = df[rand_selected_columns]\n",
    "df.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%bql CREATE TABLE \"data_full\" FROM 'temp.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of the csv file is a variable, and each row is a record. We use the `CREATE TABLE` BQL query, with the pathname of the csv file, to convert the csv data into a database table named `data_full`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all datasets have missing values, and special tokens such as `NaN` or\n",
    "`NA` indicating a particular cell is missing. In most data, empty\n",
    "strings are used. To tell BayesDB to treat empty strings as SQL `NULL` we use\n",
    "the `.nullify` command, followed by the name of the table and the string `''`\n",
    "which represents missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .nullify data_full ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further repeat the exercise for the strings `NA` and `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .nullify data_full 'NA'\n",
    "%bql .nullify data_full 'NaN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If users know about any other missing values, they should edit the cells above\n",
    "\n",
    "\n",
    "### Running basic queries on the table using BQL and SQL\n",
    "\n",
    "Now that the dataset has been loaded into at table, and missing values\n",
    "converted to `NULL`, we can run standard SQL queries to explore the contents of\n",
    "the data. For example, we can select the first 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"data_full\" LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the total number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT COUNT(*) FROM \"data_full\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling the columns in the table\n",
    "\n",
    "The full data table (`data_full`) may contains a large number of columns.\n",
    "In this notebook, our exploratory analysis will be based on a random subsample of\n",
    "100 columns. To create the subsample, we use the `.subsample_columns` magic. The\n",
    "`--keep` flag accepts a list of column names which should be kept. We will keep \n",
    "the columns denoted as experimental outcomes, since it is the identifier for\n",
    "each record. The `--seed` flag specifies\n",
    "the random seed to create the subsample, which will ensure our analyses are\n",
    "reproducible. Finally, `data_full` is the original table, `data` is\n",
    "the name of the new table, and 100 is the number of columns to downsample to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_str = ' '.join(outcomes)\n",
    "%bql .subsample_columns --seed=8 data_full data 1000 --keep  {outcomes_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a BayesDB population for data\n",
    "\n",
    "The notion of a \"population\" is a central concept in BayesDB. For a standard\n",
    "database table, such as `data`, each column is associated with a [data\n",
    "type](https://sqlite.org/datatype3.html), which in sqlite3 are `TEXT`, `REAL`,\n",
    "`INTEGER`, and `BLOB`. For a BayesDB population, each variable is associated\n",
    "with a _statistical data type_. \n",
    "\n",
    "\n",
    "These statistical types, such as `NOMINAL`,\n",
    "`NUMERICAL`, `MAGNITUDE`, and `COUNTS`, specify the set of values and default\n",
    "probability distributions used for building probabilistic models of the data in\n",
    "the population. In this tutorial, we will use the `NUMERICAL` and `NOMINAL`\n",
    "statistical data types.\n",
    "\n",
    "We can use the `GUESS SCHEMA FOR <table>` command from the Metamodeling Language\n",
    "(MML) in BayesDB to guess the statistical data types of variables in the table.\n",
    "The guesses use heuristics based on the contents in the cells. The\n",
    "`num_distinct` column shows the number of unique values for that variable, and\n",
    "the `reason` column explains which heuristic was used to make the guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml GUESS SCHEMA FOR \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this case to create a population for the `data` table. The population\n",
    "schema uses the statistical types guessed by BayesDB (from the previous cell) for all\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%mml\n",
    "CREATE POPULATION FOR \"data\" WITH SCHEMA (\n",
    "    -- Use the guesses from the previous cell for all variables.\n",
    "    GUESS STATTYPES OF (*);\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing joint distributions of data in the population\n",
    "\n",
    "Equipped with the statistical data types of variables in the population, we can\n",
    "now use the plotting features of BayesDB to produce scatter plots and heatmaps\n",
    "for the marginal and (pairwise) joint distributions of variables of interest.\n",
    "The `.interactive_pairplot` command requires a flag `--population=<pop>` for the\n",
    "population name, followed a BQL query. It generates pairplots of the data in all\n",
    "pairs of columns yielded by the query . Below, we have selected the experimental\n",
    "outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_str = ','.join(outcomes)\n",
    "%bql .interactive_pairplot --population=data SELECT {outcomes_str} FROM data_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a generator for the population using CrossCat\n",
    "\n",
    "Now that we have created the `data` population, the next step is to analyze\n",
    "the data by building probabilistic models which explain the data generating\n",
    "process. Probabilistic data analyses in BayesDB are specified by declaring\n",
    "`GENERATOR` for a population. The default generator in BayesDB is based on\n",
    "Cross-Categorization [(Crosscat)](http://jmlr.org/papers/v17/11-392.html). The\n",
    "CrossCat generator is a Bayesian factorial mixture model which learns a full\n",
    "joint distribution over all variables in the population, using a divide-and-\n",
    "conquer approach. We will explore CrossCat more in this notebook.\n",
    "\n",
    "For now we use MML to declare the a generator for the `data` population.\n",
    "Note that that we have left the schema (there are not specific model commands or\n",
    "overrides), which will apply the built-in default model discovery strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml CREATE GENERATOR FOR \"data\" USING loom;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the generator, we now need to initialize `MODELS` for the\n",
    "generator. We can think of a `GENERATOR` as specifying a hypothesis space of\n",
    "explanations for the data generating process for the population, and each\n",
    "`MODEL` is a candidate hypothesis. \n",
    "\n",
    "### Improving Crosscat hypotheses using MML `ANALYZE`\n",
    "\n",
    "We can improve the CrossCat model by using the MML `ANALYZE` command, which\n",
    "takes the name of a generator, an amount of iterations or seconds, and optional\n",
    "arguments. It then searches for improved hypotheses. \n",
    "\n",
    "The following MML command ensures the generator will have a total of 32 models\n",
    "in the ensemble (recall that we already initialized 1 model, so 31 new models\n",
    "will be added to the ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml INITIALIZE 10 MODELS IF NOT EXISTS FOR \"data\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mml ANALYZE \"data\" FOR 50 ITERATIONS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring probable dependencies between variables and comparing CrossCat\n",
    "dependence probability to linear (Pearson R) correlation\n",
    "\n",
    "As mentioned earlier, all BQL queries are aggregated across the 32 analyses in\n",
    "the ensemble. We will create a table named `dependencies` which contains the\n",
    "pairwise `DEPENDENCE PROBABILITY` values between the variables in the data. The\n",
    "value of a cell (between 0 and 1) is the fraction of analyses in the ensemble\n",
    "where those two variables are detected to be probably dependent (i.e. they are\n",
    "in the same view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE dependencies AS\n",
    "ESTIMATE\n",
    "    DEPENDENCE PROBABILITY AS \"depprob\"\n",
    "FROM PAIRWISE VARIABLES OF data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random rows from the `dependencies` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"dependencies\" ORDER BY RANDOM() LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again summarize the `dependencies` table using a heatmap. Study this\n",
    "dependence heatmap, and compare it to the heatmap produced when there was only 1\n",
    "model. Which common-sense dependencies were missed by the single model, but\n",
    "identified by the ensemble as probably dependent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap SELECT name0, name1, depprob FROM dependencies;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare dependence probabilities from CrossCat to linear (Pearson r)\n",
    "correlation values, a very common technique for finding predictive\n",
    "relationships. We can compute the Pearson R (and its p-value) in BayesDB using\n",
    "the `CORRELATION` and `CORRELATION PVALUE` queries. The following cell creates a\n",
    "table named `correlations`, which contains the R and p-value for all pairs of\n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "CREATE TABLE \"correlations\" AS\n",
    "ESTIMATE\n",
    "    CORRELATION AS \"correlation\",\n",
    "    CORRELATION PVALUE AS \"pvalue\"\n",
    "FROM PAIRWISE VARIABLES OF \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are five random rows from the `correlations` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql SELECT * FROM \"correlations\" ORDER BY RANDOM() LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Emphasis__: There is a signficiant difference between `DEPENDENCE\n",
    "PROBABILITY`, `CORRELATION`, and `CORRELATION PVALUE`. We outline these\n",
    "differences below, which will help us make comparisons between predictive\n",
    "relationships detected by CrossCat versus Pearson correlation.\n",
    "\n",
    "- `DEPENDENCE PROBABILITY`: Returns a value between [0,1] indicating the\n",
    "__probability there exists__ a predictive relationship (statistical dependence)\n",
    "between two variables.\n",
    "\n",
    "- `CORRELATION`: Returns a value between [0,1] indicating the __strength__ of\n",
    "the linear relationsip between two variables, where 0 means no linear\n",
    "correlation, and 1 means perfect linear correlation.\n",
    "\n",
    "- `CORRELATION PVALUE`: Returns a value between (0, 1) indicating the tail\n",
    "probability of the observed correlation value between two variables, under the\n",
    "null hypothesis that the two variables have zero correlation.\n",
    "\n",
    "Based on these distinctions, there is no immediate way to numerically compare\n",
    "`DEPENDENCE PROBABILITY` with `CORRELATION/CORRELATION PVALUE`. However, it is\n",
    "possible to compare the inferences about predictive relationships that each\n",
    "method gives rise to, which we do in the next section.\n",
    "\n",
    "Let us first produce a heatmap of the raw correlation values. The following\n",
    "query shows the raw correlation values (between 0 and 1) for all pairs of\n",
    "variables where the p-value is less than 0.01 (note that we are not accounting\n",
    "for multiple-testing using e.g. Bonferroni correction). Pairs of variables where\n",
    "the p-value exceeds 0.01 (and thus the null hypothesis of independence cannot be\n",
    "rejected) are shown in gray. The sparsity of the data makes it difficult to draw\n",
    "inferences about many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap SELECT name0, name1, \"correlation\" FROM \"correlations\" WHERE \"pvalue\" < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the heatmap, and compare it to the heatmap from `DEPENDENCE\n",
    "PROBABILITY`. The patterns of dependence relationships differ significantly,\n",
    "how?\n",
    "\n",
    "We can use BQL to find variables which CrossCat believes are probably dependent,\n",
    "but correlation believes are independent (either the null hypothesis of\n",
    "independence cannot be rejected, or the correlation value is significant and\n",
    "near zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    \"name0\",\n",
    "    \"name1\",\n",
    "    \"dependencies\".\"depprob\",\n",
    "    \"correlations\".\"correlation\",\n",
    "    \"correlations\".\"pvalue\"\n",
    "FROM\n",
    "    \"dependencies\"\n",
    "    JOIN \"correlations\"\n",
    "    USING (\"name0\", \"name1\")\n",
    "WHERE\n",
    "    -- CrossCat: probability dependent.\n",
    "    \"dependencies\".\"depprob\" > 0.85\n",
    "    AND (\n",
    "    -- Correlation: cannot reject null hypothesis of independence.\n",
    "    \"correlations\".\"pvalue\" > 0.05\n",
    "    OR (\n",
    "    -- Correlation: linear relationship is significant and near zero.\n",
    "    \"correlations\".\"pvalue\" < 0.05 AND \"correlations\".\"correlation\" < 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use also BQL to find variables which CrossCat believes are probably\n",
    "independent, but correlation believes are dependent (a statistically significant\n",
    "non-zero correlation value, where we are using an R cutoff of 0.15). The\n",
    "following query shows a list of such variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    \"name0\",\n",
    "    \"name1\",\n",
    "    \"dependencies\".\"depprob\",\n",
    "    \"correlations\".\"correlation\",\n",
    "    \"correlations\".\"pvalue\"\n",
    "FROM\n",
    "    \"dependencies\"\n",
    "    JOIN \"correlations\"\n",
    "    USING (\"name0\", \"name1\")\n",
    "WHERE\n",
    "    -- CrossCat: high uncertainty about dependence probability.\n",
    "    \"dependencies\".\"depprob\" < 0.05\n",
    "    AND (\n",
    "    -- Correlation: statistically significant dependence.\n",
    "    \"correlations\".\"pvalue\" < 0.05 AND \"correlations\".\"correlation\" > 0.15)\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oftentimes, we notice that linear correlation is deceived into detecting a\n",
    "dependency due to a single outlier in both cases. As a non-parametric mixture\n",
    "model, CrossCat is more robust to outliers and irregular patterns in the data,\n",
    "especially when there is insufficient evidence in the data to result in CrossCat\n",
    "reporting probable dependencies (as is the case in the two scatter plots, with\n",
    "only one data point deviating from the zero-dependence trend).\n",
    "\n",
    "Bonferroni correction for multiple testing would perhaps render the p-values of\n",
    "these correlations as statistically insignificant. However, Bonferroni is also\n",
    "highly conservative, and will cause many common-sense relationships to be\n",
    "insigificant as well under linear correlation. These design trade-offs are very\n",
    "common in drawing inferences from frequentist methods such Pearson R.\n",
    "\n",
    "Some next questions you might explore include:\n",
    "\n",
    "- For which variables do CrossCat and linear correation agree about\n",
    "dependencies?\n",
    "- Which pairs of variables have the most uncertainty about their dependence\n",
    "probability (a dependence probability value of 0.5 represents the most\n",
    "uncertainy, or a light green color)?\n",
    "\n",
    "\n",
    "### Exploring the clustering of the with respect to experimental outcomes\n",
    "\n",
    "Recall that in addition to learning a clustering of variables, CrossCat\n",
    "additionally learns a clustering of the rows within each view. These clusters\n",
    "are separated using pink lines in the CrossCat rendering. We can use the\n",
    "`SIMILARITY IN THE CONTEXT OF <variable>` query in BQL to study CrossCat's row\n",
    "partition in the view of `<variable>`.\n",
    "\n",
    "First, we subsample the table down to 1000 rows, to compute and plot similiarity of only\n",
    "1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bql\n",
    "DROP TABLE IF EXISTS subsample;\n",
    "CREATE TABLE subsample AS\n",
    "    SELECT rowid from data ORDER BY bql_rand() LIMIT 500;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heatmaps below, each row and column is a row in the data table, and the value \n",
    "of a cell (between 0 and 1) indicates the probability that those two rows are\n",
    "relevant for formulating predictions about each other.  We produces one heatmap\n",
    "for each experimental outcome. Do these clusterings make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for outcome in outcomes:\n",
    "    fig, ax = plt.subplots()\n",
    "    %bql .heatmap --label0=rowid --table=data ESTIMATE SIMILARITY IN THE CONTEXT OF {outcome} FROM PAIRWISE data\\\n",
    "        WHERE (rowid0 in (SELECT rowid FROM subsample)) AND (rowid1 in  (SELECT rowid FROM subsample));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive version\n",
    "\n",
    "We can't plot all the interactive plots automatically because the javascript objects will overwrite each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bql .interactive_heatmap --label0=rowid --table=data ESTIMATE SIMILARITY IN THE CONTEXT OF {outcomes[0]} FROM PAIRWISE data\\\n",
    "    WHERE (rowid0 in (SELECT rowid FROM subsample)) AND (rowid1 in  (SELECT rowid FROM subsample));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
